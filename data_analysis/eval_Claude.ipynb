{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T20:26:54.503476Z",
     "start_time": "2024-08-15T20:26:39.298157Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import tiktoken\n",
    "import time\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "# from openai import OpenAI\n",
    "import anthropic\n",
    "from anthropic import HUMAN_PROMPT, AI_PROMPT, Anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707f7b0c807c3293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T20:26:54.508169Z",
     "start_time": "2024-08-15T20:26:54.503118Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_LIMITS = {\n",
    "    \"gpt-3.5-turbo-0125\": 16_385,\n",
    "    \"gpt-4-turbo-2024-04-09\": 128_000,\n",
    "    \"gpt-4o-2024-05-13\": 128_000,\n",
    "    \"gpt-4o-mini-2024-07-18\": 128_000,\n",
    "    \"claude-3-5-sonnet-20240620\": 200_000,\n",
    "}\n",
    "\n",
    "# The cost per token for each model input.\n",
    "MODEL_COST_PER_INPUT = {\n",
    "    \"gpt-3.5-turbo-0125\": 0.0000005,\n",
    "    \"gpt-4-turbo-2024-04-09\": 0.00001,\n",
    "    \"gpt-4o-2024-05-13\": 0.000005,\n",
    "    \"gpt-4o-mini-2024-07-18\": 0.00000015,\n",
    "    \"claude-3-5-sonnet-20240620\": 0.000003,\n",
    "}\n",
    "\n",
    "# The cost per token for each model output.\n",
    "MODEL_COST_PER_OUTPUT = {\n",
    "    \"gpt-3.5-turbo-0125\": 0.0000015,\n",
    "    \"gpt-4-turbo-2024-04-09\": 0.00003,\n",
    "    \"gpt-4o-2024-05-13\": 0.000015,\n",
    "    \"gpt-4o-mini-2024-07-18\": 0.0000006,\n",
    "    \"claude-3-5-sonnet-20240620\":0.000015,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c11c802e901f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T20:26:54.514893Z",
     "start_time": "2024-08-15T20:26:54.510131Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If the question is a multi-choice question and you are unsure which one is correct, you must guess an option.  Please don't ask me any questions and give me the answer in the response.\n",
    "\n",
    "def call_anthropic_v2(text, image, model, client):\n",
    "    system_messages = \"You are a data analyst. I will give  you a background introduction and data analysis question. You must answer the question. \"\n",
    "    try:\n",
    "        if image:\n",
    "            base64_image = encode_image(image)\n",
    "            messages = [\n",
    "                    {\"role\": \"user\", \"content\":[{\"type\": \"image\",\n",
    "              \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/jpeg\",\n",
    "                \"data\": base64_image\n",
    "              }},\n",
    "              {\"type\": \"text\", \"text\": text}            \n",
    "            ] }]\n",
    "        else:\n",
    "            messages = [{\"role\": \"user\", \"content\": text}]\n",
    "            \n",
    "        response = client.messages.create(\n",
    "                messages=messages,\n",
    "                max_tokens=4096,\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                system=system_messages,\n",
    "            )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959b638b518acaf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T20:26:54.555039Z",
     "start_time": "2024-08-15T20:26:54.512530Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "with open(\"./data.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        samples.append(eval(line.strip()))\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41837159d54fe6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T20:26:54.562880Z",
     "start_time": "2024-08-15T20:26:54.527544Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gpt_tokenize(string: str, encoding) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def claude_tokenize(string: str, api) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = api.count_tokens(string)\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def find_jpg_files(directory):\n",
    "    jpg_files = [file for file in os.listdir(directory) if file.lower().endswith('.jpg') or file.lower().endswith('.png')]\n",
    "    return jpg_files if jpg_files else None\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "def find_excel_files(directory):\n",
    "    jpg_files = [file for file in os.listdir(directory) if (file.lower().endswith('xlsx') or file.lower().endswith('xlsb') or file.lower().endswith('xlsm')) and not \"answer\" in file.lower()]\n",
    "    return jpg_files if jpg_files else None\n",
    "\n",
    "def read_excel(file_path):\n",
    "    # 读取Excel文件中的所有sheet\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    sheets = {}\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        sheets[sheet_name] = xls.parse(sheet_name)\n",
    "    return sheets\n",
    "\n",
    "def dataframe_to_text(df):\n",
    "    # 将DataFrame转换为文本\n",
    "    text = df.to_string(index=False)\n",
    "    return text\n",
    "\n",
    "def combine_sheets_text(sheets):\n",
    "    # 将所有sheet的文本内容组合起来\n",
    "    combined_text = \"\"\n",
    "    for sheet_name, df in sheets.items():\n",
    "        sheet_text = dataframe_to_text(df)\n",
    "        combined_text += f\"Sheet name: {sheet_name}\\n{sheet_text}\\n\\n\"\n",
    "    return combined_text\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def truncate_text(text, max_tokens=128000):\n",
    "    # 计算当前文本的token数\n",
    "    tokens = text.split()\n",
    "    if len(tokens) > max_tokens:\n",
    "        # 截断文本以确保不超过最大token数\n",
    "        text = ' '.join(tokens[-max_tokens:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd0212ea2e1e824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:05:43.034870Z",
     "start_time": "2024-08-15T16:05:41.591442Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/messages)', 'type': 'invalid_request_error', 'param': '', 'code': ''}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANTHROPIC_BASE_URL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.xiaoai.plus\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAnthropic()\n\u001b[0;32m----> 7\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-sonnet-20240620\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a world-class poet. Respond only with short poems.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhy is the ocean salty?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(message)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsbench/lib/python3.10/site-packages/anthropic/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsbench/lib/python3.10/site-packages/anthropic/resources/messages.py:860\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_given(timeout) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m==\u001b[39m DEFAULT_TIMEOUT:\n\u001b[1;32m    859\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m\n\u001b[0;32m--> 860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/chat/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsbench/lib/python3.10/site-packages/anthropic/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsbench/lib/python3.10/site-packages/anthropic/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsbench/lib/python3.10/site-packages/anthropic/_base_client.py:1040\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1039\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1043\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1044\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1049\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/messages)', 'type': 'invalid_request_error', 'param': '', 'code': ''}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "\n",
    "os.environ['ANTHROPIC_AUTH_TOKEN'] = 'your-token-id'\n",
    "os.environ['ANTHROPIC_BASE_URL'] = 'https://api.xiaoai.plus'\n",
    "client = anthropic.Anthropic()\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=\"You are a world-class poet. Respond only with short poems.\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Why is the ocean salty?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77cb93d65ebe82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47e26d1c169390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T05:18:43.391486Z",
     "start_time": "2024-08-16T05:18:43.025017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your_api = \"\"\n",
    "# client = Anthropic(api_key=your_api)\n",
    "# model = \"claude-3-5-sonnet-20240620\"\n",
    "# from transformers import GPT2TokenizerFast\n",
    "\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7ecfeafaa4e02",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-15T20:41:16.776583Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = call_anthropic_v2(\"你好\",None, model, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8290931dba6d2a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T20:41:16.785161Z",
     "start_time": "2024-08-15T20:41:16.779396Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acf35fbe1b0929",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path = './data'\n",
    "# model = \"gpt-3.5-turbo-0125\"\n",
    "# model = \"gpt-4o-mini-2024-07-18\"\n",
    "# total_cost = 65.39828\n",
    "total_cost = 0\n",
    "# encoding = tiktoken.encoding_for_model(model)\n",
    "## record 3\n",
    "for id in tqdm(range(0, len(samples))):\n",
    "    # print(sample)\n",
    "    sample =samples[id]\n",
    "    if len(sample[\"questions\"]) > 0:\n",
    "        start = sample[\"questions\"][0]\n",
    "        end = sample[\"questions\"][-1]\n",
    "        # print(start)\n",
    "        # print(end)\n",
    "        image = find_jpg_files(os.path.join(data_path, sample[\"id\"]))\n",
    "        if image:\n",
    "            image = os.path.join(data_path, sample[\"id\"], image[0])\n",
    "        \n",
    "        excel_content = \"\"\n",
    "        excels = find_excel_files(os.path.join(data_path, sample[\"id\"]))\n",
    "        if excels:\n",
    "            for excel in excels:\n",
    "                excel_file_path = os.path.join(data_path,  sample[\"id\"], excel)\n",
    "                # print(excel_file_path)\n",
    "                sheets = read_excel(excel_file_path)\n",
    "                combined_text = combine_sheets_text(sheets)\n",
    "                excel_content += f\"The excel file {excel} is: \" + combined_text\n",
    "\n",
    "        introduction = read_txt(os.path.join(data_path, sample[\"id\"], \"introduction.txt\"))\n",
    "        questions = []\n",
    "        for question_name in sample[\"questions\"]:\n",
    "            questions.append(read_txt(os.path.join(data_path, sample[\"id\"], question_name+\".txt\")))\n",
    "            \n",
    "        # print(workbooks)\n",
    "        \n",
    "        text = \"\"\n",
    "        if excel_content:\n",
    "            text += f\"The workbook is detailed as follows. {excel_content} \\n\"\n",
    "        text += f\"The introduction is detailed as follows. \\n {introduction} \\n\"\n",
    "        answers = []\n",
    "        \n",
    "        line_num = 0\n",
    "        save_path = os.path.join(\"./modeloff/evaluation/save_process\", model)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        if os.path.exists(os.path.join(save_path, sample['id']+\".json\")):\n",
    "            with open(os.path.join(save_path, sample['id']+\".json\"), \"r\") as f:\n",
    "                for line in f:\n",
    "                    line_num += 1\n",
    "        print(f\"existing {line_num} answers\")\n",
    "        save_f =  open(os.path.join(save_path, sample['id']+\".json\"), \"a+\")\n",
    "        \n",
    "        \n",
    "        for question in tqdm(questions[line_num:]):\n",
    "            prompt = text +  f\"The questions are detailed as follows. \\n {question}\"\n",
    "        \n",
    "            # print(len(encoding.encode(prompt)))\n",
    "            cut_text = tokenizer.decode(tokenizer.encode(prompt)[6000-MODEL_LIMITS[model]:])\n",
    "            # print(len(encoding.encode(prompt)))\n",
    "            # print(prompt)\n",
    "            # text = truncate_text(text, 20000)\n",
    "            try:\n",
    "                while True:\n",
    "                    start = time.time()\n",
    "                    response = call_anthropic_v2(cut_text, image, model, client)\n",
    "                    cost = response.usage.output_tokens * MODEL_COST_PER_OUTPUT[model] + response.usage.input_tokens * MODEL_COST_PER_INPUT[model]\n",
    "                    ans = {\"id\": sample[\"id\"], \"model\": response.model, \"input\": response.usage.input_tokens,\n",
    "                                    \"output\": response.usage.output_tokens, \"cost\": cost, \"time\": time.time()-start, \"response\": response.content[0].text}\n",
    "                    answers.append({\"id\": sample[\"id\"], \"model\": response.model, \"input\": response.usage.input_tokens,\n",
    "                                    \"output\": response.usage.output_tokens, \"cost\": cost, \"time\": time.time()-start, \"response\": response.content[0].text})\n",
    "                    total_cost += cost\n",
    "                    print(\"Total cost: \", total_cost)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"error: {e}\")\n",
    "                time.sleep(10)\n",
    "            json.dump(ans, save_f)\n",
    "            save_f.write(\"\\n\")\n",
    "            save_f.flush()\n",
    "            # time.sleep(60)\n",
    "            # break\n",
    "        save_f.close()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803b607afe48d9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723a58557b8e528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T06:36:33.642558Z",
     "start_time": "2024-08-15T06:36:33.640760Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8211fdbcdd33fdf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response.usage.o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44e2b9c6a8c0a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(response.usage.completion_tokens)\n",
    "print(response.usage.prompt_tokens)\n",
    "print(response.choices[0].message.content)\n",
    "print(response.model)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fe6837f36ce53",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path = './data'\n",
    "# model = \"gpt-3.5-turbo-0125\"\n",
    "# model = \"gpt-4o-mini-2024-07-18\"\n",
    "# total_cost = 65.39828\n",
    "total_cost = 0\n",
    "# encoding = tiktoken.encoding_for_model(model)\n",
    "## record 3\n",
    "for id in tqdm(range(0, len(samples))):\n",
    "    # print(sample)\n",
    "    sample =samples[id]\n",
    "    if len(sample[\"questions\"]) > 0:\n",
    "        start = sample[\"questions\"][0]\n",
    "        end = sample[\"questions\"][-1]\n",
    "        # print(start)\n",
    "        # print(end)\n",
    "        image = find_jpg_files(os.path.join(data_path, sample[\"id\"]))\n",
    "        if image:\n",
    "            image = os.path.join(data_path, sample[\"id\"], image[0])\n",
    "        \n",
    "        excel_content = \"\"\n",
    "        excels = find_excel_files(os.path.join(data_path, sample[\"id\"]))\n",
    "        if excels:\n",
    "            for excel in excels:\n",
    "                excel_file_path = os.path.join(data_path,  sample[\"id\"], excel)\n",
    "                # print(excel_file_path)\n",
    "                sheets = read_excel(excel_file_path)\n",
    "                combined_text = combine_sheets_text(sheets)\n",
    "                excel_content += f\"The excel file {excel} is: \" + combined_text\n",
    "\n",
    "        introduction = read_txt(os.path.join(data_path, sample[\"id\"], \"introduction.txt\"))\n",
    "        questions = []\n",
    "        for question_name in sample[\"questions\"]:\n",
    "            questions.append(read_txt(os.path.join(data_path, sample[\"id\"], question_name+\".txt\")))\n",
    "            \n",
    "        # print(workbooks)\n",
    "        \n",
    "        text = \"\"\n",
    "        if excel_content:\n",
    "            text += f\"The workbook is detailed as follows. {excel_content} \\n\"\n",
    "        text += f\"The introduction is detailed as follows. \\n {introduction} \\n\"\n",
    "        answers = []\n",
    "        for question in questions:\n",
    "            prompt = text +  f\"The questions are detailed as follows. \\n {question}\"\n",
    "        \n",
    "            # print(len(encoding.encode(prompt)))\n",
    "            cut_text = tokenizer.decode(tokenizer.encode(prompt)[6000-MODEL_LIMITS[model]:])\n",
    "            # print(len(encoding.encode(prompt)))\n",
    "            # print(prompt)\n",
    "            # text = truncate_text(text, 20000)\n",
    "            start = time.time()\n",
    "            response = call_anthropic_v2(cut_text, image, model, client)\n",
    "            cost = response.usage.output_tokens * MODEL_COST_PER_OUTPUT[model] + response.usage.input_tokens * MODEL_COST_PER_INPUT[model]\n",
    "            \n",
    "            answers.append({\"id\": sample[\"id\"], \"model\": response.model, \"input\": response.usage.input_tokens,\n",
    "                            \"output\": response.usage.output_tokens, \"cost\": cost, \"time\": time.time()-start, \"response\": response.content[0].text})\n",
    "            total_cost += cost\n",
    "            print(\"Total cost: \", total_cost)\n",
    "            # time.sleep(60)\n",
    "            # break\n",
    "        save_path = os.path.join(\"./evaluation/save_process\", model)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        with open(os.path.join(save_path, sample['id']+\".json\"), \"w\") as f:\n",
    "            for answer in answers:\n",
    "                json.dump(answer, f)\n",
    "                f.write(\"\\n\")\n",
    "        # break\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
